<!DOCTYPE html>
<html class="no-js" lang="zh-cn">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【LLM】VLLM 单卡部署 Qwen3-4B - Zhu Xingda</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="上下而求索" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">上下而求索</div>
					<div class="logo__tagline">朱兴达的个人博客</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">菜单</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">
				
				<span class="menu__text">主页</span>
				
			</a>
		</li>
		<li class="menu__item menu__item--active">
			<a class="menu__link" href="/tech/">
				
				<span class="menu__text">技术</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/note/">
				
				<span class="menu__text">笔记本</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/life/">
				
				<span class="menu__text">生活</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/%E5%85%B3%E4%BA%8E%E6%88%91/">
				
				<span class="menu__text">关于我</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【LLM】VLLM 单卡部署 Qwen3-4B</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 1 1 0 28 1 1 0 0 1 0-28m0 3a3 3 0 1 0 0 22 3 3 0 0 0 0-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class="meta__text" datetime="2025-05-23T13:44:34&#43;08:00">2025-05-23</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/llm/" rel="category">LLM</a>
	</span>
</div></div>
		</header>
		
		
		<div class="content post__content clearfix">
			<p>尝试在个人显卡上部署一个 Qwen-4B 模型</p>
<h2 id="1-部署">1. 部署</h2>
<h4 id="11-软硬件版本">1.1 软硬件版本</h4>
<ul>
<li>显卡：RTX4070</li>
<li>操作系统：Ubuntu 23.10</li>
<li>Python：3.12</li>
<li>VLLM：0.8.5.post1</li>
<li>CUDA：12.2</li>
</ul>
<h4 id="12-部署环境配置">1.2 部署环境配置</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 下载模型文件</span>
</span></span><span style="display:flex;"><span>$ git clone https://www.modelscope.cn/Qwen/Qwen3-4B.git
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建 python 环境</span>
</span></span><span style="display:flex;"><span>$ conda create --name llm_hello_world python<span style="color:#f92672">=</span>3.12
</span></span><span style="display:flex;"><span>$ conda activate llm_hello_world
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 安装 vllm</span>
</span></span><span style="display:flex;"><span>$ pip install -U <span style="color:#e6db74">&#34;vllm&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Docker 拉 Open-WebUI 镜像</span>
</span></span><span style="display:flex;"><span>$ docker pull ghcr.dockerproxy.com/open-webui/open-webui:main
</span></span></code></pre></div><h4 id="13-部署测试">1.3 部署测试</h4>
<p>启动 VLLM Serve</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ vllm serve /home/zhuxingda/Projets/Qwen3-4B --quantization fp8 --gpu-memory-utilization 0.6 --max-num-seq <span style="color:#ae81ff">1</span> --max-model-len 4K --api-key <span style="color:#ae81ff">123456</span>
</span></span></code></pre></div><p>部署完之后请求接口确认成功</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ curl http://localhost:8000/v1/models -H <span style="color:#e6db74">&#34;Authorization: Bearer 123456&#34;</span> | jq
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;object&#34;</span>: <span style="color:#e6db74">&#34;list&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;data&#34;</span>: <span style="color:#f92672">[</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;/home/zhuxingda/Projets/Qwen3-4B&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;object&#34;</span>: <span style="color:#e6db74">&#34;model&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;created&#34;</span>: 1748317788,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;owned_by&#34;</span>: <span style="color:#e6db74">&#34;vllm&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;root&#34;</span>: <span style="color:#e6db74">&#34;/home/zhuxingda/Projets/Qwen3-4B&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;parent&#34;</span>: null,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;max_model_len&#34;</span>: 4096,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;permission&#34;</span>: <span style="color:#f92672">[</span>
</span></span><span style="display:flex;"><span>       ...
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>启动 Open-WebUI 镜像</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ sudo docker run -d <span style="color:#ae81ff">\ </span>                                                      
</span></span><span style="display:flex;"><span>   --restart unless-stopped <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   --name open-webui <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   -p 0.0.0.0:8080:8080 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   -v <span style="color:#66d9ef">$(</span>pwd<span style="color:#66d9ef">)</span>/data:/app/backend/data <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   -e WEBUI_SECRET_KEY<span style="color:#f92672">=</span><span style="color:#ae81ff">123456</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   -e HF_ENDPOINT<span style="color:#f92672">=</span>https://hf-mirror.com <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   ghcr.nju.edu.cn/open-webui/open-webui:main
</span></span></code></pre></div><p>启动成功之后打开 http://host-name:8080 并在设置中新建连接
<img alt="截图" src="https://ooo.0x0.ooo/2025/05/27/OdVgYK.png">
然后在工作空间中添加模型即可使用<br>
查看显卡监控发现尽管是个 4B 模型并且用了 FP8 量化，但 VLLM 的进程还是占用了9个多GB显存</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ nvidia-smi 
</span></span><span style="display:flex;"><span>Tue May <span style="color:#ae81ff">27</span> 12:32:42 <span style="color:#ae81ff">2025</span>       
</span></span><span style="display:flex;"><span>+---------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |
</span></span><span style="display:flex;"><span>|-----------------------------------------+----------------------+----------------------+
</span></span><span style="display:flex;"><span>| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
</span></span><span style="display:flex;"><span>| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
</span></span><span style="display:flex;"><span>|                                         |                      |               MIG M. |
</span></span><span style="display:flex;"><span>|<span style="color:#f92672">=========================================</span>+<span style="color:#f92672">======================</span>+<span style="color:#f92672">======================</span>|
</span></span><span style="display:flex;"><span>|   <span style="color:#ae81ff">0</span>  NVIDIA GeForce RTX <span style="color:#ae81ff">4070</span>        Off | 00000000:01:00.0  On |                  N/A |
</span></span><span style="display:flex;"><span>|  0%   37C    P8               7W / 200W |   9858MiB / 12282MiB |      0%      Default |
</span></span><span style="display:flex;"><span>|                                         |                      |                  N/A |
</span></span><span style="display:flex;"><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style="display:flex;"><span>                                                                                         
</span></span><span style="display:flex;"><span>+---------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| Processes:                                                                            |
</span></span><span style="display:flex;"><span>|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
</span></span><span style="display:flex;"><span>|        ID   ID                                                             Usage      |
</span></span><span style="display:flex;"><span>|<span style="color:#f92672">=======================================================================================</span>|
</span></span><span style="display:flex;"><span>|    <span style="color:#ae81ff">0</span>   N/A  N/A    <span style="color:#ae81ff">213900</span>      C   ...envs/llm_hello_world/bin/python3.12     9852MiB |
</span></span><span style="display:flex;"><span>+---------------------------------------------------------------------------------------+
</span></span></code></pre></div><p>但查看 VLLM 启动时的日志模型只占用了 4.1 GB</p>
<blockquote>
<p>INFO 05-25 23:37:44 [gpu_model_runner.py:1347] Model loading took 4.1402 GiB and 0.962489 seconds</p>
</blockquote>
		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 0 0 0-6 3 3 0 0 0 0 6"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/vllm/" rel="tag">VLLM</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/qwen3/" rel="tag">Qwen3</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div id="gitalk-container"></div>
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script>
  const gitalk = new Gitalk({
    clientID: '5e216fcfdd8cdb462b85',
    clientSecret: '10659215d325c41513a047c07f576e345e697b3b',
    repo: 'zhuxingda.github.io',
    owner: 'ZhuXingda',
    admin: ['ZhuXingda'],
    id: location.pathname, 
    distractionFreeMode: false 
  });
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('gitalk-container').innerHTML = 'Gitalk comments not available by default when the website is previewed locally.';
      return;
    }
    gitalk.render('gitalk-container');
  })();
</script>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/tech/paimon_file_structure/" rel="prev">
			<span class="pager__subtitle">«&thinsp;上一篇</span>
			<p class="pager__title">【Paimon】文件结构</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/tech/data_warehouse_model_theory/" rel="next">
			<span class="pager__subtitle">下一篇&thinsp;»</span>
			<p class="pager__title">【数据仓库】数仓建模理论</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2025 Zhu Xingda.
			<span class="footer__copyright-credits">基于 <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> 引擎和 <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> 主题</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>